## 大数据常见面试题

### 大数据问题套路

````
分布式问题第一步一般是大化小,大化小策略有:
	利用hash函数将文件分到多个机器中去处理
	利用hash函数将文件分成多个小文件(只有一台机器)
	利用数据大小,分成若干区间
大化小的目的有两个,一方面是因为内存不够一次不能处理完;还有一个原因是化小的过程中就可以排除掉无效数据
````



### 常见的计算

Integer.MAX_VALUE：20亿(2^31 - 1)

内存1G = 10亿B(字节)



### 布隆过滤器原理

创建一个长度为m的bit数组,然后针对每个黑名单中的URL(假设样本量为N),将其hash函数散列到的位置置1,这样的hash函数有K个,最终得到了一个bit数组,数组中有若干位是1,其他位是0.

对于目标URL,使用同样的步骤进行hash散列,如果散列到的所有数都是1,则认为这个URL是应该被拦截的.它不会漏杀,但是会误杀.误杀概率P和m,N,K的大小都有关系.



### 一致性hash

分布式集群部署时,采用一致性hash去发送请求,有利于集群的拓展和高可用效率.其中用到了虚拟节点技术

比如当需要拓展集群中的机器时,使用一致性hash的结构,只用改动少数服务器机器的数据即可.某个机器宕机时,它丢给其他机器分担的请求大小也不至于太大.



### 40亿数中出现次数最多的数

找到40亿个数中, 出现次数最多的数, 内存限制2G

每个数的大小范围最大有40亿，每个数的次数范围最大有40亿，int是20亿，所以需要用long(8B)，一个map两个long也就是(16B)，最多会有40亿个map(每个数都不同)，就是640亿B，也就是64G，一次计算显然是不够的。

对数据进行分组可以解决，因为一个map已经可以表示任意一个数出现任意次，2G内存有20亿B，一次可以计算1亿个map(16亿B)，所以将40亿数先分hash成40组，如果某组元素个数大于1亿，再进行hash分组。用hash分组是因为同一个数会被分到同一个组里



### 百亿数据中出现次数top100的数据

百度搜索每天搜索词条达100亿,怎么求这100亿数据中频次top100

```
因为是统计频次,相同的数据用hash散列肯定散到一起,所以先hash函数将文件散列到不同机器中,如果某个机器的数据量还是太大,再用hash函数散列成不同的小文件.遍历小文件,统计map,然后将map利用value建立大根堆.对所有小文件的top100进行排序,得到机器的top100.再用同样方法处理机器的top100,得到最终的top100
```



### 40亿数中没出现的数

找到40亿个数中, 0~40亿范围内出现次数为0的数,内存限制1G

````
统计出现最多的数时,需要用map,但是统计有没有出现只用一个变量就可以了.但是如果用4B的Set,40亿数据最多是16G,显然内存不够.因为表示有没有出现用4B很浪费,所以用bit数组表示即可.创建一个40亿长度的bit数组,数组为1表示出现过,为0表示没出现过.数组长度消耗内存为5亿B,即500MB
````

#### 进阶1:内存10MB,但是只用找到一个没出现的数

````
核心思想是使用区间数组:用一个长度为m的数组(数组元素4B)来表示区间计数,遍历40亿个数,为其所在区间计数+1
比如m=4000,第一个区间数组消耗的内存很少.那么数组中每个元素表示的区间大小是100万,遍历这个数组,只要这个数组中某个元素小于100万,表明这个区间中一定有某个数没出现. 用一个100万长度的bit数组遍历即可,这个数组的内存消耗也是少于10MB的
````

#### 进阶2:找到所有出现两次的数,内存1G

````
出现0次/出现非0次用1个bit即可表示,出现2次/非2次需要用2个bit表示:00->0次,01->1次,10->两次,11->大于2
因此要用80亿bit长度的数组,内存消耗刚好为1G
````



### 40亿数的中位数

找到40亿数的中位数,内存限制1GB

`````
利用区间数组,创建一个长度为2048的数组,数组中每个元素(大小为4B)表示区间大小为40亿/2048个数出现的次数
然后再遍历区间数组,找到数组累加和最后一次小于20亿的区间a(假设累加和为suma)和第一次大于20亿的区间b,则剩下的工作就是在区间b中找到第20亿-suma个数.
这时候用map(一个元素8B)记录这个区间中每个数出现的次数,大小为8B*40亿/2048. 如果不满足要求,继续扩大区间数组的长度即可,因为扩大它长度带来的内存消耗很少,但是却可以大大减少最后map数组消耗的内存
`````

